---
title: Blueprint Authoring Guide
description: Advanced guide for creating custom Renku blueprints and producers
---

This comprehensive guide explains how to author Renku blueprints: the YAML schema, how producers compose into workflows, and the rules the planner and runner enforce.

## Overview

Blueprints are YAML files that define complete video generation workflows. This guide is for advanced users who want to:

- Create custom blueprints for specific use cases
- Build reusable producers for new AI models
- Understand the internal workings of the planner and runner

If you're new to Renku, start with the [Quick Start](/docs/quick-start) guide first.

## Core Concepts

### Blueprint vs Producer

**Blueprints** are workflow definitions that:
- Define user-facing inputs and final artifacts
- Import and connect multiple producers
- Define loops for parallel execution
- Specify collectors for fan-in aggregation

**Producers** are execution units that:
- Accept inputs and produce artifacts
- Map inputs to specific AI model parameters
- Support multiple model variants
- Can be reused across multiple blueprints

### Data Flow

Data flows through the system via connections:

```
Blueprint Input → Producer Input → Producer → Artifact → Next Producer Input
                                                   │
                                                   └→ Blueprint Artifact
```

For looped producers:
```
ScriptProducer.NarrationScript[0] → AudioProducer[0].TextInput
ScriptProducer.NarrationScript[1] → AudioProducer[1].TextInput
ScriptProducer.NarrationScript[2] → AudioProducer[2].TextInput
```

---

## Blueprint YAML Reference

### Complete Schema

```yaml
meta:
  name: <string>           # Human-readable name (required)
  description: <string>    # Purpose and behavior
  id: <string>             # Unique identifier in PascalCase (required)
  version: <semver>        # Semantic version (e.g., 0.1.0)
  author: <string>         # Creator name
  license: <string>        # License type (e.g., MIT)

inputs:
  - name: <string>         # Input identifier in PascalCase (required)
    description: <string>  # Purpose and usage
    type: <string>         # Data type (required)
    required: <boolean>    # Whether mandatory (default: true)

artifacts:
  - name: <string>         # Artifact identifier in PascalCase (required)
    description: <string>  # Purpose and content
    type: <string>         # Output type (required)
    itemType: <string>     # Element type for arrays
    countInput: <string>   # Input that determines array size

loops:
  - name: <string>         # Loop identifier in lowercase (required)
    description: <string>  # Purpose and behavior
    countInput: <string>   # Input that determines iteration count (required)
    countInputOffset: <int> # Offset added to count
    parent: <string>       # Parent loop for nesting

producers:
  - name: <string>         # Producer instance name in PascalCase (required)
    producer: <string>     # Qualified name: "category/name" (preferred)
    path: <string>         # Relative path to producer YAML (for custom producers)
    loop: <string>         # Loop dimension(s) (e.g., "segment")

connections:
  - from: <string>         # Source reference (required)
    to: <string>           # Target reference (required)
    if: <string>           # Condition name for conditional execution

conditions:
  <conditionName>:
    when: <string>         # Path to artifact field (e.g., Producer.Artifact.Field[dim])
    is: <any>              # Equals this value
    isNot: <any>           # Does not equal this value
    contains: <string>     # String contains value
    greaterThan: <number>  # Greater than
    lessThan: <number>     # Less than
    exists: <boolean>      # Field exists and is truthy
    matches: <string>      # Regex pattern
    all: <array>           # AND: all sub-conditions must pass
    any: <array>           # OR: at least one sub-condition must pass

collectors:
  - name: <string>         # Collector identifier (required)
    from: <string>         # Source artifact reference (required)
    into: <string>         # Target input reference (required)
    groupBy: <string>      # Loop dimension for grouping (required)
    orderBy: <string>      # Loop dimension for ordering
```

### Input Types

| Type | Description |
|------|-------------|
| `string` | Text value |
| `int` | Integer number |
| `image` | Image file reference |
| `audio` | Audio file reference |
| `video` | Video file reference |
| `json` | Structured JSON data |
| `collection` | Array of items (used with `fanIn: true`) |

### Artifact Types

| Type | Description |
|------|-------------|
| `string` | Text output |
| `json` | Structured JSON with virtual property expansion (see [JSON Artifacts](#json-artifacts)) |
| `image` | Image file |
| `audio` | Audio file |
| `video` | Video file |
| `array` | Array of items (requires `itemType`) |
| `multiDimArray` | Multi-dimensional array |

### Example Blueprint

```yaml
meta:
  name: Video Only Narration
  description: Generate video segments from a textual inquiry.
  id: Video
  version: 0.1.0
  author: Renku
  license: MIT

inputs:
  - name: InquiryPrompt
    description: The prompt describing the movie script.
    type: string
    required: true
  - name: Duration
    description: Desired duration in seconds.
    type: int
    required: true
  - name: NumOfSegments
    description: Number of narration segments.
    type: int
    required: true
  - name: Style
    description: Visual style for the video.
    type: string
    required: true

artifacts:
  - name: SegmentVideo
    description: Generated video for each segment.
    type: array
    itemType: video
    countInput: NumOfSegments

loops:
  - name: segment
    description: Iterates over narration segments.
    countInput: NumOfSegments

producers:
  - name: ScriptProducer
    producer: prompt/script
  - name: VideoPromptProducer
    producer: prompt/video
  - name: VideoProducer
    producer: asset/text-to-video
    loop: segment

connections:
  # Wire inputs to ScriptProducer
  - from: InquiryPrompt
    to: ScriptProducer.InquiryPrompt
  - from: Duration
    to: ScriptProducer.Duration
  - from: NumOfSegments
    to: ScriptProducer.NumOfSegments

  # Wire script to VideoPromptProducer (looped)
  - from: ScriptProducer.NarrationScript[segment]
    to: VideoPromptProducer[segment].NarrativeText
  - from: Style
    to: VideoPromptProducer[segment].Style

  # Wire prompts to VideoProducer (looped)
  - from: VideoPromptProducer.VideoPrompt[segment]
    to: VideoProducer[segment].Prompt

  # Wire output to blueprint artifact
  - from: VideoProducer[segment].SegmentVideo
    to: SegmentVideo[segment]
```

---

## Producer YAML Reference

### Complete Schema

```yaml
meta:
  name: <string>           # Human-readable name (required)
  description: <string>    # Purpose and behavior
  id: <string>             # Unique identifier in PascalCase (required)
  version: <semver>        # Semantic version
  author: <string>         # Creator name
  license: <string>        # License type
  promptFile: <string>     # Path to TOML prompt config (LLM producers only)
  outputSchema: <string>   # Path to JSON schema for structured output (LLM producers only)

inputs:
  - name: <string>         # Input identifier (required)
    description: <string>  # Purpose and usage
    type: <string>         # Data type (required)
    fanIn: <boolean>       # Is this a fan-in collection input
    dimensions: <string>   # Dimension labels (e.g., "segment")

artifacts:
  - name: <string>         # Artifact identifier (required)
    description: <string>  # Purpose and content
    type: <string>         # Output type (required)
    itemType: <string>     # Element type for arrays
    countInput: <string>   # Input for array size

mappings:
  <provider>:              # Provider name: replicate, fal-ai, openai, renku
    <model>:               # Model identifier
      <ProducerInput>: <providerField>           # Simple mapping
      <ProducerInput>:                           # Object form
        field: <providerField>
        transform:                               # Value transformation
          <inputValue>: <providerValue>
        expand: <boolean>                        # Spread object into payload
        combine:                                 # Combine multiple inputs
          inputs: [<input1>, <input2>]
          table:
            "<val1>+<val2>": <result>
        conditional:                             # Conditional mapping
          when:
            input: <inputName>
            notEmpty: <boolean>
          then: <mapping>
```

**Note on LLM Producers:** For producers that use LLMs (OpenAI, etc.), the `promptFile` and `outputSchema` fields are defined in the `meta:` section of the producer YAML file. These are intrinsic to the producer's functionality.

### Example: Script Producer (OpenAI)

```yaml
meta:
  name: Script Generation
  description: Generate documentary scripts.
  id: ScriptProducer
  version: 0.1.0
  promptFile: ./script.toml              # Prompt template for LLM
  outputSchema: ./script-output.json     # JSON schema for structured output

inputs:
  - name: InquiryPrompt
    type: string
  - name: Duration
    type: int
  - name: NumOfSegments
    type: int
  - name: Audience
    type: string

artifacts:
  - name: MovieTitle
    type: string
  - name: MovieSummary
    type: string
  - name: NarrationScript
    type: array
    itemType: string
    countInput: NumOfSegments
```

**Note:** LLM producers define `promptFile` and `outputSchema` in the `meta:` section. The model selection (e.g., `gpt-4o`) is specified in the input template file.

### Example: Video Producer (with Mappings)

```yaml
meta:
  name: Video Generation
  id: VideoProducer
  version: 0.1.0

inputs:
  - name: Prompt
    type: string
  - name: AspectRatio
    type: string
  - name: Resolution
    type: string

artifacts:
  - name: SegmentVideo
    type: video

mappings:
  replicate:
    bytedance/seedance-1-pro-fast:
      Prompt: prompt
      AspectRatio: aspect_ratio
      Resolution: resolution
    google/veo-3.1-fast:
      Prompt: prompt
      AspectRatio: aspect_ratio
  fal-ai:
    bytedance/seedream/v4.5/text-to-image:
      Prompt: prompt
      AspectRatio:
        field: image_size
        transform:
          "16:9": landscape_16_9
          "9:16": portrait_16_9
          "1:1": square_hd
```

### Example: Timeline Composer (Fan-In)

```yaml
meta:
  name: Timeline Composer
  id: TimelineComposer
  version: 0.1.0

inputs:
  - name: VideoSegments
    type: collection
    itemType: video
    dimensions: segment
    fanIn: true
  - name: AudioSegments
    type: collection
    itemType: audio
    dimensions: segment
    fanIn: true
  - name: Duration
    type: int

artifacts:
  - name: Timeline
    type: json

# Timeline composer uses renku provider - no field mappings needed
# Model selection and config are specified in the input template
```

---

## Connections

Connections define data flow between nodes.

### Direct Connections

```yaml
connections:
  - from: InquiryPrompt
    to: ScriptProducer.InquiryPrompt
```

### Array Indexing

```yaml
connections:
  - from: ScriptProducer.NarrationScript[segment]
    to: AudioProducer[segment].TextInput
```

Expands to:
- `NarrationScript[0]` → `AudioProducer[0].TextInput`
- `NarrationScript[1]` → `AudioProducer[1].TextInput`
- etc.

### Multi-Dimensional Indexing (Nested Loops)

When you have nested loops (a loop with a `parent`), use multiple indices:

```yaml
# Define nested loops
loops:
  - name: segment
    countInput: NumOfSegments
  - name: image
    parent: segment                    # image is nested inside segment
    countInput: NumOfImagesPerNarrative

# Producer runs for each segment × image combination
producers:
  - name: ImageProducer
    producer: asset/text-to-image
    loop: segment.image                # Dot notation for nested loops

# Connections use multiple indices
connections:
  - from: ImagePromptProducer.ImagePrompt[segment][image]
    to: ImageProducer[segment][image].Prompt
  - from: ImageProducer[segment][image].GeneratedImage
    to: SegmentImage[segment][image]
```

If `NumOfSegments = 3` and `NumOfImagesPerNarrative = 2`, this creates 6 producer instances: `[0][0]`, `[0][1]`, `[1][0]`, `[1][1]`, `[2][0]`, `[2][1]`.

### Offset Selectors

```yaml
connections:
  - from: ImageProducer[image].SegmentImage
    to: ImageToVideoProducer[segment].InputImage1
  - from: ImageProducer[image+1].SegmentImage
    to: ImageToVideoProducer[segment].InputImage2
```

Creates sliding window patterns. For N segments, you need N+1 images.

### Indexed Collection Binding

When a producer has a collection input, you can connect different artifacts to specific indices:

```yaml
connections:
  # Different artifacts bound to specific collection indices
  - from: CharacterImageProducer.GeneratedImage
    to: VideoProducer[clip].ReferenceImages[0]
  - from: ProductImageProducer.GeneratedImage
    to: VideoProducer[clip].ReferenceImages[1]
```

This pattern is useful when:
- A producer accepts multiple reference images as a collection
- Each reference image comes from a different upstream producer
- The same images should be used for all loop instances

**How it works:**
1. The planner creates element-level input bindings: `ReferenceImages[0]` → artifact ID
2. At runtime, the SDK reconstructs the array from these element-level bindings
3. The producer receives `ReferenceImages: [CharacterImage, ProductImage]`

**Comparison with whole-collection binding:**

| Pattern | Syntax | Use Case |
|---------|--------|----------|
| Whole-collection | `AllImages → ReferenceImages` | Connect an entire array artifact |
| Element-level | `Image1 → ReferenceImages[0]` | Connect individual artifacts to specific indices |

### Broadcast Connections

A scalar input broadcasts to all loop instances:

```yaml
connections:
  - from: Style
    to: VideoPromptProducer[segment].Style
```

### Connecting to JSON Artifact Properties

When a producer outputs a `type: json` artifact with a schema, you can connect to nested properties using dot-path syntax:

```yaml
connections:
  # Connect to top-level property
  - from: DocProducer.VideoScript.Title
    to: TitleRenderer.Title

  # Connect to array element property
  - from: DocProducer.VideoScript.Segments[segment].Script
    to: AudioProducer[segment].TextInput

  # Connect to nested array property
  - from: DocProducer.VideoScript.Segments[segment].ImagePrompts[image].Prompt
    to: ImageProducer[segment][image].Prompt
```

See [JSON Artifacts](#json-artifacts) for full details on defining JSON artifacts with schemas.

### Conditional Connections

Connections can be made conditional based on runtime values from upstream artifacts. This enables dynamic workflow branching where different producers execute depending on the data produced earlier in the pipeline.

#### Defining Conditions

Define named conditions in the `conditions:` section of your blueprint:

```yaml
conditions:
  isImageNarration:
    when: DocProducer.VideoScript.Segments[segment].NarrationType
    is: "ImageNarration"
  isAudioNeeded:
    any:
      - when: DocProducer.VideoScript.Segments[segment].NarrationType
        is: "TalkingHead"
      - when: DocProducer.VideoScript.Segments[segment].UseNarrationAudio
        is: true
  isTalkingHead:
    when: DocProducer.VideoScript.Segments[segment].NarrationType
    is: "TalkingHead"
```

#### Condition Path Format

The `when` field references a path to a value in an upstream artifact:

```
<Producer>.<Artifact>.<FieldPath>[dimension]
```

- **Producer**: The producer that creates the artifact (e.g., `DocProducer`)
- **Artifact**: The artifact name (e.g., `VideoScript`)
- **FieldPath**: Dot-separated path to the field (e.g., `Segments[segment].NarrationType`)
- **Dimensions**: Use dimension placeholders like `[segment]` for per-instance evaluation

#### Condition Operators

| Operator | Description |
|----------|-------------|
| `is` | Equals the specified value |
| `isNot` | Does not equal the specified value |
| `contains` | String contains the value |
| `greaterThan` | Greater than (numeric) |
| `lessThan` | Less than (numeric) |
| `greaterOrEqual` | Greater than or equal (numeric) |
| `lessOrEqual` | Less than or equal (numeric) |
| `exists` | Field exists and is truthy |
| `matches` | Matches a regular expression |

#### Condition Groups

Combine multiple conditions with `all` (AND) or `any` (OR):

```yaml
conditions:
  needsAudio:
    any:
      - when: DocProducer.VideoScript.Segments[segment].NarrationType
        is: "TalkingHead"
      - when: DocProducer.VideoScript.Segments[segment].UseNarrationAudio
        is: true

  isHighQuality:
    all:
      - when: DocProducer.VideoScript.Segments[segment].Quality
        is: "high"
      - when: DocProducer.VideoScript.Segments[segment].Duration
        greaterThan: 10
```

#### Using Conditions on Connections

Reference a named condition using the `if:` attribute:

```yaml
connections:
  # ImageProducer only runs when NarrationType is "ImageNarration"
  - from: DocProducer.VideoScript.Segments[segment].ImagePrompts[image]
    to: ImageProducer[segment][image].Prompt
    if: isImageNarration

  # AudioProducer runs when TalkingHead OR UseNarrationAudio is true
  - from: DocProducer.VideoScript.Segments[segment].Script
    to: AudioProducer[segment].TextInput
    if: isAudioNeeded
```

#### Runtime Behavior

When a condition is evaluated at runtime:

1. **Condition Evaluation**: The runner resolves the artifact data and evaluates the condition for each dimension instance
2. **Input Filtering**: If a condition is not satisfied, that input is filtered out
3. **Job Skipping**: If ALL conditional inputs for a job are not satisfied, the job is **skipped**
4. **Artifact Absence**: Skipped jobs produce no artifacts - those artifact IDs are absent from the manifest

**Example**: With 3 segments where `NarrationType = ["ImageNarration", "TalkingHead", "ImageNarration"]`:
- `ImageProducer[0]` and `ImageProducer[2]` execute (ImageNarration)
- `ImageProducer[1]` is skipped (TalkingHead)
- `AudioProducer[1]` executes (TalkingHead)
- `AudioProducer[0]` and `AudioProducer[2]` may be skipped (unless UseNarrationAudio=true)

---

## JSON Artifacts

JSON artifacts store structured data as a single blob while exposing nested properties as virtual artifacts for granular connections. This enables:

- **Granular connections**: Wire individual properties to downstream producers
- **Efficient caching**: Only re-run downstream jobs when specific properties change
- **Schema validation**: Structured output from LLMs with JSON schema enforcement

### Defining JSON Artifacts in Producers

```yaml
meta:
  id: DocumentaryPromptProducer
  name: Documentary Script Generation
  promptFile: ./documentary-prompt.toml           # Prompt template
  outputSchema: ./documentary-prompt-output.json  # JSON schema for validation

inputs:
  - name: InquiryPrompt
    type: string
  - name: NumOfSegments
    type: int
  - name: NumOfImagesPerSegment
    type: int
    default: 1

artifacts:
  - name: VideoScript
    type: json
    description: The generated video script
    arrays:
      - path: Segments
        countInput: NumOfSegments
      - path: Segments.ImagePrompts
        countInput: NumOfImagesPerSegment
```

**Note:** The model selection (e.g., `gpt-4o`) is specified in the input template file, not in the producer.

### The `arrays` Field

The `arrays` field maps JSON array paths to input variables that determine their sizes:

```yaml
arrays:
  - path: Segments                    # Top-level array
    countInput: NumOfSegments         # Sized by NumOfSegments input
  - path: Segments.ImagePrompts       # Nested array within each segment
    countInput: NumOfImagesPerSegment # Sized by NumOfImagesPerSegment input
```

This enables the planner to:
1. Create dimension placeholders for arrays (e.g., `[segment]`, `[image]`)
2. Expand virtual artifacts for each array element
3. Track granular changes for incremental re-runs

### Schema Association

The `outputSchema` from the producer's `meta:` section is automatically associated with `type: json` artifacts. The schema defines the structure of the JSON output:

```json
{
  "name": "VideoScript",
  "strict": true,
  "schema": {
    "type": "object",
    "properties": {
      "Title": { "type": "string" },
      "Segments": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "Script": { "type": "string" },
            "ImagePrompts": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "Prompt": { "type": "string" }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

### Virtual Artifact Paths

The planner expands JSON schemas into virtual artifacts that can be referenced in connections:

| Virtual Artifact ID | Description |
|---------------------|-------------|
| `Producer.VideoScript.Title` | Top-level string property |
| `Producer.VideoScript.Segments[segment].Script` | Script for each segment |
| `Producer.VideoScript.Segments[segment].ImagePrompts[image].Prompt` | Image prompt for each segment/image |

### Granular Dirty Tracking

Each virtual artifact gets its own content hash. When you edit a JSON artifact:

- Only virtual artifacts whose content actually changed are marked dirty
- Only downstream jobs consuming those specific properties re-run
- Other downstream jobs use cached results

**Example**: If you edit only `Segments[1].Script`, only `AudioProducer[1]` re-runs. `AudioProducer[0]` and `AudioProducer[2]` use cached results.

---

## Loops and Dimensions

### Basic Loop

```yaml
loops:
  - name: segment
    countInput: NumOfSegments
```

If `NumOfSegments = 3`, looped producers run 3 times.

### Loop with Offset

```yaml
loops:
  - name: image
    countInput: NumOfSegments
    countInputOffset: 1
```

Count is `NumOfSegments + 1`. Use for sliding window patterns.

### Nested Loops

```yaml
loops:
  - name: segment
    countInput: NumOfSegments
  - name: image
    parent: segment
    countInput: NumOfImagesPerSegment
```

Creates two-dimensional iteration. If `NumOfSegments = 3` and `NumOfImagesPerSegment = 2`, you get 6 instances.

### Assigning Producers to Loops

```yaml
producers:
  - name: ScriptProducer
    producer: prompt/script
    # No loop - runs once

  - name: AudioProducer
    producer: asset/text-to-speech
    loop: segment
    # Runs per segment

  - name: ImageProducer
    producer: asset/text-to-image
    loop: segment.image
    # Runs per segment × image
```

---

## Collectors and Fan-In

Collectors aggregate multiple artifacts for downstream processing.

:::caution[Critical: Fan-In Requires BOTH Connection AND Collector]
A common mistake is defining only a collector. **Fan-in inputs require BOTH:**

```yaml
# ❌ WRONG - Only collector (fan-in input will be empty!)
collectors:
  - name: TimelineImages
    from: ImageProducer[segment][image].GeneratedImage
    into: TimelineComposer.ImageSegments
    groupBy: segment

# ✅ CORRECT - Both connection AND collector
connections:
  - from: ImageProducer[segment][image].GeneratedImage
    to: TimelineComposer.ImageSegments  # ← REQUIRED!

collectors:
  - name: TimelineImages
    from: ImageProducer[segment][image].GeneratedImage
    into: TimelineComposer.ImageSegments
    groupBy: segment
    orderBy: image
```

**Why both?** The connection creates the data flow edge; the collector defines grouping/ordering.
:::

### Basic Collector

```yaml
collectors:
  - name: TimelineVideo
    from: VideoProducer[segment].SegmentVideo
    into: TimelineComposer.VideoSegments
    groupBy: segment
```

### Collector with Ordering

```yaml
collectors:
  - name: TimelineImages
    from: ImageProducer[segment][image].SegmentImage
    into: TimelineComposer.ImageSegments
    groupBy: segment
    orderBy: image
```

### Fan-In Inputs

The target input must have `fanIn: true`:

```yaml
inputs:
  - name: VideoSegments
    type: collection
    itemType: video
    dimensions: segment
    fanIn: true
```

---

## Canonical IDs

Canonical IDs are fully qualified node identifiers.

### Format

```
Type:path.to.name[index0][index1]...
```

### Examples

| ID | Description |
|----|-------------|
| `Input:InquiryPrompt` | Blueprint input |
| `Input:ScriptProducer.Duration` | Producer input |
| `Artifact:VideoProducer.SegmentVideo[0]` | First video |
| `Artifact:ImageProducer.SegmentImage[2][1]` | Image at [2][1] |
| `Producer:AudioProducer[0]` | First AudioProducer instance |

---

## Planner and Runner

### Planner Process

1. **Load blueprint tree** - Blueprint + all producer imports
2. **Build graph** - Nodes for inputs, artifacts, producers
3. **Resolve dimensions** - Calculate loop sizes
4. **Expand instances** - Create concrete instances per dimension
5. **Align edges** - Match dimension indices
6. **Build execution layers** - Topological sort into parallel groups

### Runner Process

1. **Execute layer by layer** - Jobs in same layer can run in parallel
2. **Resolve artifacts** - Load upstream data for each job
3. **Materialize fan-in** - Group and order collection items
4. **Invoke providers** - Call AI APIs
5. **Store artifacts** - Persist to blob storage

### Dirty Tracking

For incremental runs:
1. Compare input hashes against manifest
2. Find changed or missing artifacts
3. Mark affected jobs as dirty
4. Propagate through dependency graph
5. Only execute dirty jobs

---

## Input Files Reference

### Structure

```yaml
inputs:
  InquiryPrompt: "Your topic here"
  Duration: 60
  NumOfSegments: 3
  Style: "Documentary"
  AspectRatio: "16:9"

models:
  - model: gpt-5-mini
    provider: openai
    producerId: ScriptProducer
    config:
      text_format: json_schema
  - model: minimax/speech-2.6-hd
    provider: replicate
    producerId: AudioProducer
  - model: bytedance/seedance-1-pro-fast
    provider: replicate
    producerId: VideoProducer
  - model: timeline/ordered
    provider: renku
    producerId: TimelineComposer
    config:
      tracks: ["Video", "Audio"]
      masterTracks: ["Audio"]
```

### Model Selection

Select which model to use for each producer. The `config` field passes provider-specific options:

```yaml
models:
  # LLM with structured output
  - model: gpt-5-mini
    provider: openai
    producerId: ScriptProducer
    config:
      text_format: json_schema

  # Video generation
  - model: google/veo-3.1-fast
    provider: replicate
    producerId: VideoProducer

  # Timeline composition with custom config
  - model: timeline/ordered
    provider: renku
    producerId: TimelineComposer
    config:
      tracks: ["Video", "Audio", "Music"]
      masterTracks: ["Audio"]
      musicClip:
        volume: 0.4
```

**Note:** Input-to-provider field mappings are defined in the producer YAML's `mappings:` section, not in the input template. This keeps input files simple.

---

## Validation Rules

### Blueprint Validation

| Rule | Error |
|------|-------|
| Meta section required | `Blueprint must have a meta section` |
| Meta.id required | `Blueprint meta must have an id` |
| At least one artifact | `Blueprint must declare at least one artifact` |
| Cannot mix models and producers | `Cannot define both models and producer imports` |

### Input Validation

| Rule | Error |
|------|-------|
| Optional inputs need defaults | `Optional input must declare a default value` |
| Input name required | `Input must have a name` |
| Input type required | `Input must have a type` |

### Connection Validation

| Rule | Error |
|------|-------|
| Valid dimension syntax | `Invalid dimension selector` |
| Known dimension | `Unknown loop symbol` |
| Dimension count match | `Inconsistent dimension counts` |

---

## Common Patterns

### Audio-Only Narration

```yaml
loops:
  - name: segment
    countInput: NumOfSegments

producers:
  - name: ScriptProducer
    producer: prompt/script
  - name: AudioProducer
    producer: asset/text-to-speech
    loop: segment

connections:
  - from: ScriptProducer.NarrationScript[segment]
    to: AudioProducer[segment].TextInput
  - from: AudioProducer[segment].SegmentAudio
    to: SegmentAudio[segment]
```

### Image-to-Video (Sliding Window)

```yaml
loops:
  - name: segment
    countInput: NumOfSegments
  - name: image
    countInput: NumOfSegments
    countInputOffset: 1  # N+1 images

producers:
  - name: ImageProducer
    producer: asset/text-to-image
    loop: image
  - name: ImageToVideoProducer
    producer: asset/image-to-video
    loop: segment

connections:
  - from: ImageProducer[image].SegmentImage
    to: ImageToVideoProducer[segment].InputImage1
  - from: ImageProducer[image+1].SegmentImage
    to: ImageToVideoProducer[segment].InputImage2
```

### Full Timeline

```yaml
producers:
  - name: VideoProducer
    producer: asset/text-to-video
    loop: segment
  - name: AudioProducer
    producer: asset/text-to-speech
    loop: segment
  - name: TimelineComposer
    producer: composition/timeline-composer

collectors:
  - name: TimelineVideo
    from: VideoProducer[segment].SegmentVideo
    into: TimelineComposer.VideoSegments
    groupBy: segment
  - name: TimelineAudio
    from: AudioProducer[segment].SegmentAudio
    into: TimelineComposer.AudioSegments
    groupBy: segment
```

---

## Debugging

### Validate Blueprint

```bash
renku blueprints:validate ./my-blueprint.yaml
```

### Dry Run

```bash
renku generate --inputs=./inputs.yaml --blueprint=./blueprint.yaml --dry-run
```

### Inspect Execution Plan

```bash
cat {builds}/{movie}/runs/rev-0001-plan.json
```

### Common Issues

| Problem | Cause | Solution |
|---------|-------|----------|
| Fan-in empty | Missing collector | Add collector from source to target |
| Dimension error | Mismatched dimensions | Verify source/target dimensions match |
| Optional input error | Missing default | Add `default:` to optional inputs |
| Missing artifacts | Wrong build path | Check `dist/` per package |

---

## Directory Structure

### Catalog Organization

```
catalog/
├── blueprints/
│   ├── audio-only/
│   │   ├── audio-only.yaml
│   │   └── input-template.yaml
│   └── video-only/
│       ├── video-only.yaml
│       └── input-template.yaml
├── producers/
│   ├── asset/                        # Media generation producers
│   │   ├── text-to-image.yaml
│   │   ├── text-to-video.yaml
│   │   ├── text-to-speech.yaml
│   │   └── image-to-video.yaml
│   ├── prompt/                       # LLM-based producers
│   │   ├── script/
│   │   │   ├── script.yaml
│   │   │   └── script.toml
│   │   └── video/
│   │       └── video.yaml
│   └── composition/                  # Composition producers
│       └── timeline-composer.yaml
└── models/
    └── *.yaml
```

**Producer Categories:**
- `asset/` - Media generation: images, video, audio, music
- `prompt/` - LLM-based script and prompt generation
- `composition/` - Timeline composition and video export

### Naming Conventions

| Item | Convention | Example |
|------|------------|---------|
| Blueprint files | kebab-case | `image-to-video.yaml` |
| Producer files | kebab-case | `script.yaml` |
| IDs | PascalCase | `id: ImageToVideo` |
| Loop names | lowercase | `name: segment` |
| Input/Artifact names | PascalCase | `name: InquiryPrompt` |

### Producer References

**Preferred: Qualified names** resolve from the catalog:

```yaml
# Works from any location - resolves from catalog
producers:
  - name: ScriptProducer
    producer: prompt/script        # → catalog/producers/prompt/script/script.yaml
  - name: VideoProducer
    producer: asset/text-to-video  # → catalog/producers/asset/text-to-video.yaml
```

**Legacy: Relative paths** for custom local producers:

```yaml
# For custom producers not in the catalog
producers:
  - name: MyCustomProducer
    path: ./my-producers/custom.yaml  # Relative to blueprint file
```
